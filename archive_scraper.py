import os
import json
import time
import requests
import google.generativeai as genai
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
CHANNEL_ID = os.environ.get("CHANNEL_ID")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
YOUR_CHANNEL_USERNAME = "peidano"

START_YEAR = 2015
END_YEAR = 2025
TOP_N_MONTHLY = 25
STATE_FILE = "archive_state.json"

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-2.5-flash-lite') 

MONTHS = {
    1: "January", 2: "February", 3: "March", 4: "April", 5: "May", 6: "June",
    7: "July", 8: "August", 9: "September", 10: "October", 11: "November", 12: "December"
}

def load_state():
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, "r") as f:
                return json.load(f)
        except: pass
    return {"year": START_YEAR, "month": 1, "product_idx": 0, "status": "MONTHLY"}

def save_state(state):
    with open(STATE_FILE, "w") as f:
        json.dump(state, f)
    try:
        os.system('git config --global user.email "bot@github.com"')
        os.system('git config --global user.name "Archive Bot"')
        os.system(f'git add {STATE_FILE}')
        os.system(f'git commit -m "Update state"')
        os.system('git push')
    except: pass

def generate_content(product_name, original_desc, maker_comment, launch_date):
    print(f"   Generating AI content for {product_name}...", flush=True)
    combined_text = f"Product: {product_name}\nDescription: {original_desc}\nMaker Comment: {maker_comment}"

    prompt_pitch = f"""
    Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„:
    {combined_text}
    
    ÙˆØ¸ÛŒÙÙ‡: ØªÙˆ Ø³Ø±Ø¯Ø¨ÛŒØ± Ø§Ø±Ø´Ø¯ Ú©Ø§Ù†Ø§Ù„ Peidano Ù‡Ø³ØªÛŒ. Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ù†.
    Ù‚ÙˆØ§Ù†ÛŒÙ†:
    1. Ù„Ø­Ù† Ø±Ø§ÙˆÛŒ: Ø³ÙˆÙ… Ø´Ø®Øµ (Ø¯Ø§Ù†Ø§ÛŒ Ú©Ù„).
    2. Ù…Ø­ØªÙˆØ§: Ú†ÛŒØ³ØªØŸ Ú†Ù‡ Ù…Ø´Ú©Ù„ÛŒ Ø±Ø§ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ
    3. Ø§Ú¯Ø± Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ù†Ø§Ù‚Øµ Ø§Ø³ØªØŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… Ù…Ø­ØµÙˆÙ„ Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©ÙˆØªØ§Ù‡ØŒ Ø­Ø¯Ø³ Ø¨Ø²Ù† Ú©Ø§Ø±Ø´ Ú†ÛŒØ³Øª (Ø§Ù…Ø§ Ø¯Ø±ÙˆØº Ù†Ú¯Ùˆ).
    4. Ø·ÙˆÙ„: 5 ØªØ§ 10 Ø®Ø·.
    5. Ø²Ø¨Ø§Ù†: ÙØ§Ø±Ø³ÛŒ Ø±ÙˆØ§Ù†.
    """
    try:
        pitch_res = model.generate_content(prompt_pitch).text.strip()
        time.sleep(2)
    except: pitch_res = "ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."

    prompt_history = f"""
    Ù…Ø­ØµÙˆÙ„: {product_name} ({launch_date})
    ØªÙˆØ¶ÛŒØ­Ø§Øª: {original_desc[:300]}...
    ÙˆØ¸ÛŒÙÙ‡: ØªØ­Ù„ÛŒÙ„ Ú©ÙˆØªØ§Ù‡ (3 ØªØ§ 5 Ø®Ø·) ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ.
    1. Ø¨Ø§ Ø³Ø±Ú† ÛŒØ§ Ø¯Ø§Ù†Ø´ Ø®ÙˆØ¯Øª: Ø§Ù„Ø§Ù† Ú©Ø¬Ø§Ø³ØªØŸ (ÙØ¹Ø§Ù„/Ø´Ú©Ø³Øªâ€ŒØ®ÙˆØ±Ø¯Ù‡/ÙØ±ÙˆØ®ØªÙ‡ Ø´Ø¯Ù‡)
    2. Ù…Ø¯Ù„ Ø¯Ø±Ø¢Ù…Ø¯ÛŒØŸ
    3. Ø´Ø±ÙˆØ¹ Ø¨Ø§: "Ø¬Ù…Ù†Ø§ÛŒ: ..."
    """
    try:
        history_res = model.generate_content(prompt_history).text.strip()
        time.sleep(2)
    except: history_res = "Ø¬Ù…Ù†Ø§ÛŒ: Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ§Ø±ÛŒØ®ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."
    return pitch_res, history_res

def send_to_telegram(data):
    print(f"   Sending to Telegram...", flush=True)
    caption = f"""
ğŸ—“ï¸ {data['date_str']}

{data['hashtags']}

**{data['title']}**

{data['pitch_text']}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{data['history_text']}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
**Product Hunt:** [View Page]({data['ph_link']})
**Website:** [Visit Site]({data['website']})
**Channel:** @{YOUR_CHANNEL_USERNAME}
"""
    media = []
    images = data['images'][:10]
    
    if not images:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
        requests.post(url, json={"chat_id": CHANNEL_ID, "text": caption, "parse_mode": "Markdown"})
        return

    for i, img in enumerate(images):
        media_item = {"type": "photo", "media": img}
        if i == 0:
            media_item["caption"] = caption
            media_item["parse_mode"] = "Markdown"
        media.append(media_item)

    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMediaGroup"
    requests.post(url, json={"chat_id": CHANNEL_ID, "media": media})

def run_scraper():
    state = load_state()
    current_run_month = state['month']
    
    print(f"ğŸš€ Starting scraper. Target: {state['year']}/{state['month']} - Start Index: {state['product_idx']}", flush=True)

    with sync_playwright() as p:
        # Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† Ø³ÙˆØ±Ø³ HTML
        browser = p.chromium.launch(headless=True, args=['--no-sandbox'])
        page = browser.new_page()

        while state['month'] == current_run_month:
            year = state['year']
            month = state['month']
            if year >= END_YEAR: break

            url = f"https://www.producthunt.com/leaderboard/monthly/{year}/{month}"
            print(f"ğŸ“„ Opening List: {url}", flush=True)
            
            try:
                page.goto(url, timeout=60000, wait_until="commit") # ÙÙ‚Ø· ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø§ÙˆÙ„ÛŒÙ‡
                time.sleep(5) # Ú©Ù…ÛŒ ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø±Ù†Ø¯Ø± Ø§ÙˆÙ„ÛŒÙ‡
                
                # Ø¯Ø±ÛŒØ§ÙØª HTML Ú©Ø§Ù…Ù„ Ù„ÛŒØ³Øª
                list_html = page.content()
                soup = BeautifulSoup(list_html, 'html.parser')

                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ (Ø±ÙˆØ´ data-test Ú©Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ø´Ù…Ø§ Ø¨ÙˆØ¯)
                items = soup.select('[data-test^="post-item-"]')
                
                # ÙØ§Ù„â€ŒØ¨Ú© Ø¨Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ú¯Ø± data-test Ù†Ø¨ÙˆØ¯
                if not items:
                    print("âš ï¸ data-test not found. Fallback to links.", flush=True)
                    links = soup.find_all('a', href=True)
                    valid_items = []
                    seen = set()
                    for link in links:
                        href = link['href']
                        if ("/posts/" in href or "/products/" in href) and "#" not in href:
                            full = "https://www.producthunt.com" + href if href.startswith("/") else href
                            if full not in seen:
                                valid_items.append({"url": full, "element": link})
                                seen.add(full)
                    items = valid_items
                else:
                    # ØªØ¨Ø¯ÛŒÙ„ data-test Ù‡Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
                    parsed_items = []
                    for item in items:
                        link_tag = item.find('a', href=True)
                        if link_tag:
                            href = link_tag['href']
                            full = "https://www.producthunt.com" + href if href.startswith("/") else href
                            parsed_items.append({"url": full, "element": item})
                    items = parsed_items

                items = items[:TOP_N_MONTHLY]
                print(f"   Found {len(items)} products.", flush=True)
                
                if not items:
                    print("âŒ No items found. Skipping month.", flush=True)
                    state['month'] += 1
                    if state['month'] > 12:
                        state['month'] = 1
                        state['year'] += 1
                    save_state(state)
                    break

                # --- Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø­ØµÙˆÙ„ ØªÚ©ÛŒ ---
                current_idx = state['product_idx']
                if current_idx >= len(items):
                    print("   Month finished! Next.", flush=True)
                    state['month'] += 1
                    state['product_idx'] = 0
                    if state['month'] > 12:
                        state['month'] = 1
                        state['year'] += 1
                    save_state(state)
                    break

                item_data = items[current_idx]
                ph_link = item_data['url']
                
                # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† ØªÛŒØªØ± Ø§Ø² Ù„ÛŒØ³Øª
                raw_title = item_data['element'].get_text(separator=" ", strip=True)
                # Ù…Ø¹Ù…ÙˆÙ„Ø§ ØªÛŒØªØ± Ø¨Ø®Ø´ Ø§ÙˆÙ„ Ù…ØªÙ†Ù‡
                title = raw_title.split("  ")[0] if "  " in raw_title else raw_title[:30] 

                print(f"ğŸ” Processing: {title} ({ph_link})", flush=True)

                # --- Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† HTML) ---
                page.goto(ph_link, timeout=60000, wait_until="commit")
                time.sleep(4)
                detail_html = page.content()
                detail_soup = BeautifulSoup(detail_html, 'html.parser')

                # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÛŒØªØ± Ø¯Ù‚ÛŒÙ‚ (h1)
                h1 = detail_soup.find('h1')
                if h1: title = h1.get_text(strip=True)

                # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ¨Ø³Ø§ÛŒØª
                website = ph_link
                visit_btn = detail_soup.find('a', attrs={'data-test': 'visit-button'})
                if visit_btn: website = visit_btn.get('href')

                # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª
                desc = title
                # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ØªØ§ÛŒ ØªÙˆØ¶ÛŒØ­Ø§Øª
                meta_desc = detail_soup.find('meta', attrs={'name': 'description'})
                if meta_desc: 
                    desc = meta_desc.get('content')
                else:
                    # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø¨Ø¯Ù†Ù‡
                    desc_div = detail_soup.find('div', class_=lambda x: x and 'description' in x)
                    if desc_div: desc = desc_div.get_text(strip=True)

                # 4. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù†Øª Ø³Ø§Ø²Ù†Ø¯Ù‡
                maker_comment = ""
                comment_div = detail_soup.find('div', class_=lambda x: x and 'commentBody' in x)
                if comment_div: maker_comment = comment_div.get_text(strip=True)

                # 5. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ú©Ø³â€ŒÙ‡Ø§
                images = []
                # Ù‡Ù…Ù‡ Ø¹Ú©Ø³â€ŒÙ‡Ø§ÛŒ ØµÙØ­Ù‡ Ú©Ù‡ Ø³ÙˆØ±Ø³ Ù…Ø¹ØªØ¨Ø± Ø¯Ø§Ø±Ù†Ø¯
                img_tags = detail_soup.find_all('img')
                for img in img_tags:
                    src = img.get('src') or img.get('srcset')
                    if src and "http" in src and "avatar" not in src and "logo" not in src:
                        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¹Ú©Ø³â€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú© (Ø¢ÛŒÚ©ÙˆÙ†â€ŒÙ‡Ø§)
                        if "width" in img.attrs and int(img['width']) > 100:
                             images.append(src.split(' ')[0]) # Ù‡Ù†Ø¯Ù„ Ú©Ø±Ø¯Ù† srcset
                        elif "media" in str(img.parent): # Ø¹Ú©Ø³â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ Ú¯Ø§Ù„Ø±ÛŒ Ù…Ø¹Ù…ÙˆÙ„Ø§ ØªÙˆÛŒ Ù…Ø¯ÛŒØ§ Ù‡Ø³ØªÙ†
                             images.append(src.split(' ')[0])
                
                # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
                images = list(set(images))
                if not images: # Ø§Ú¯Ø± Ø¹Ú©Ø³ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø¹Ú©Ø³ Ø§ÙˆØ¬ÛŒ (OG Image) Ø±Ùˆ Ø¨Ú¯ÛŒØ±
                    og_img = detail_soup.find('meta', property='og:image')
                    if og_img: images.append(og_img['content'])

                hashtags = "#Tech" # ØªÚ¯ Ù¾ÛŒØ´ÙØ±Ø¶
                
                # ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø§Ø±Ø³Ø§Ù„
                date_str = f"{MONTHS[month]} {year}"
                pitch_text, history_text = generate_content(title, desc, maker_comment, date_str)
                
                post_data = {
                    "title": title, "date_str": date_str, "hashtags": hashtags,
                    "pitch_text": pitch_text, "history_text": history_text,
                    "ph_link": ph_link, "website": website, "images": images
                }
                
                send_to_telegram(post_data)
                print(f"âœ… Sent.", flush=True)
                
                state['product_idx'] += 1
                save_state(state)
                time.sleep(5)

            except Exception as e:
                print(f"âŒ Error processing item: {e}", flush=True)
                # Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ… Ø®Ø±Ø§Ø¨ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ú¯ÛŒØ± Ú©Ø±Ø¯Ù†
                state['product_idx'] += 1
                save_state(state)
                time.sleep(5)

if __name__ == "__main__":
    run_scraper()
